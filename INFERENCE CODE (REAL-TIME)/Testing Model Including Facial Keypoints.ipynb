{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from model import Net\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "def LoadModel(fpath):\n",
    "    '''\n",
    "    function to load saved model\n",
    "    '''\n",
    "    c = torch.load(fpath, map_location='cpu')\n",
    "    model = c['model']\n",
    "    model.load_state_dict(c['state_dict'])\n",
    "    # as we've to perform testing, we don't need backpropagation so setting 'requires_grad' as false\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    # model.eval() ->  .eval() does not change any behaviour of gradient calculations , but are used to set specific layers\n",
    "    #                  like dropout and batchnorm to evaluation mode i.e. dropout layer won't drop activations and \n",
    "    #                  batchnorm will use running estimates instead of batch statistics.\n",
    "    return model.eval()\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                                        transforms.Resize((224,224)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# Initializing file paths for both the models\n",
    "fpath1 = 'Real-Time Face Mask Detection Model.pth'\n",
    "fpath2 = 'Facial Keypoints Model.pt'\n",
    "\n",
    "# Loading the models for testing\n",
    "model = LoadModel(fpath1)\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(fpath2))\n",
    "for parameter in net.parameters():\n",
    "    parameter.requires_grad = False\n",
    "net.eval()\n",
    "model_lm = net\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "detector = MTCNN()\n",
    "\n",
    "# Accessing the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "f = cv2.FONT_HERSHEY_DUPLEX\n",
    "t = 2\n",
    "red = (0,0,255)\n",
    "green = (0,255,0)\n",
    "blue = (255,255,0)\n",
    "yellow = (0,155,255)\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    # getting the frame in 'frm' and a bool value in 'ret' which is true if a frame is returned\n",
    "    ret, frm = cap.read()\n",
    "    if ret == True:\n",
    "        # converting into grayscale for feature reduction and grayscale images are less computation intensive to operate on\n",
    "        gray = cv2.cvtColor(frm, cv2.COLOR_BGR2GRAY)\n",
    "        col = cv2.cvtColor(frm, cv2.COLOR_BGR2RGB)\n",
    "        # detecting the faces in the frame returned, it will return the coords of bounding box along with its height and width\n",
    "        result = detector.detect_faces(col)\n",
    "        for box in result:\n",
    "            x, y, w, h = box['box']\n",
    "            keypoints = box['keypoints']\n",
    "            # drawing the bounding box based on the coordinates provided by haar_cascade\n",
    "            cv2.rectangle(frm, (x,y), (x+w,y+h), 2)\n",
    "            # cropping the portion of image covered by the bounding box\n",
    "            crp = Image.fromarray(frm,mode = 'RGB')\n",
    "            #cropped_img = frm[y:y+h, x:x+w]\n",
    "            cropped_img = crp.crop((x,y,x+w,y+h))\n",
    "            s = (w*h)/(50000)\n",
    "            if s<0.5:\n",
    "                s=0.5\n",
    "            pil_image = train_transforms(cropped_img)\n",
    "            image = pil_image.unsqueeze(0)\n",
    "            # feeding the test cropped image into the model\n",
    "            result = model(image)\n",
    "            img = np.array(image)\n",
    "            img = img[:,0,:,:]\n",
    "            img = img.reshape(img.shape[0], 1, img.shape[1], img.shape[2])\n",
    "            result_lm = model_lm(torch.from_numpy(img))\n",
    "            result_lm = np.array(result_lm)\n",
    "            result_lm = result_lm*(0.19*h)\n",
    "            result_lm = result_lm.reshape(68,2)\n",
    "            result_lm[:,0] += x+(0.28*h)\n",
    "            result_lm[:,1] += y+(0.49*w)\n",
    "            _, maximum = torch.max(result.data, 1)\n",
    "            pred = maximum.item()\n",
    "            # displaying results based on classification\n",
    "            if pred == 0:\n",
    "                cv2.circle(frm, (keypoints['left_eye']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['right_eye']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['nose']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['mouth_left']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['mouth_right']), 2, yellow, 2)\n",
    "                (lw,lh), bl = cv2.getTextSize(\"Correctly Masked\", f, s, t)\n",
    "                cv2.putText(frm, \"Correctly Masked\", ((int(((w+x)-x-lw)/2)+x),y-10), f, s, green, t)\n",
    "                cv2.rectangle(frm, (x,y), (x+w,y+h), green, 2)  # green colour rectangle if mask is worn correctly\n",
    "            elif pred == 1:\n",
    "                cv2.circle(frm, (keypoints['left_eye']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['right_eye']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['nose']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['mouth_left']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['mouth_right']), 2, yellow, 2)\n",
    "                (lw,lh), bl = cv2.getTextSize(\"Unmasked\", f, s, t)\n",
    "                cv2.putText(frm, \"Unmasked\", ((int(((w+x)-x-lw)/2)+x),y-10), f, s, red, t)\n",
    "                cv2.rectangle(frm, (x,y), (x+w,y+h), red, 2)   # red colour rectangle if mask is not being worn\n",
    "            elif pred == 2:\n",
    "                cv2.circle(frm, (keypoints['left_eye']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['right_eye']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['nose']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['mouth_left']), 2, yellow, 2)\n",
    "                cv2.circle(frm, (keypoints['mouth_right']), 2, yellow, 2)\n",
    "                (lw,lh), bl = cv2.getTextSize(\"Incorrectly Masked\", f, s, t)\n",
    "                cv2.putText(frm, \"Incorrectly Masked\", ((int(((w+x)-x-lw)/2)+x),y-10), f, s, blue, t)\n",
    "                cv2.rectangle(frm, (x,y), (x+w,y+h), blue, 2)   # blue colour rectangle if mask is not worn correctly\n",
    "        cv2.imshow('frame',frm)\n",
    "        if (cv2.waitKey(1) & 0xFF) == ord('q'):  # press 'q' to exit\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
