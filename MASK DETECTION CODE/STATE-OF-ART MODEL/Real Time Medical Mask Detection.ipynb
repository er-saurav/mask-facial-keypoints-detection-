{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Medical Mask Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2 # OpenCV\n",
    "import matplotlib.pyplot as plt\n",
    "import xmltodict # PyPI library that treats XMLs as JSON files (containing key values pairs)\n",
    "import torch # PyTorch\n",
    "import torchvision # PyTorch library containing useful attributes and functionalities\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def ImageNames():\n",
    "    '''\n",
    "    function to get the names of images in the dataset\n",
    "    '''\n",
    "    imgs = []\n",
    "    for dname, _, fname in os.walk('/IIT-K Project/Dataset/images/'):\n",
    "        for f in fname:\n",
    "            fpath = os.path.join(dname,f)\n",
    "            ext = fpath[len(fpath)-4:]\n",
    "            if ext != '.xml':\n",
    "                imgs.append(f)\n",
    "    return imgs\n",
    "def Path(img):\n",
    "    '''\n",
    "    function to get the path of the images and their labels in the dataset\n",
    "    '''\n",
    "    o = '/IIT-K Project/Dataset/'\n",
    "    i = o + 'images/' + img\n",
    "    if img[-4:] == 'jpeg':\n",
    "        lbl = img[:-5] + '.xml'\n",
    "    else: # 'jpg' or 'png'\n",
    "        lbl = img[:-4] + '.xml'\n",
    "    l = o + 'annotations/' + lbl\n",
    "    return i, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ParseXML(l):\n",
    "    '''\n",
    "    function to convert the labels of XML files to dictionary format and return the classes of dataset along with bounding boxes\n",
    "    '''\n",
    "    x = xmltodict.parse(open(l,'rb'))\n",
    "    items = x['annotation']['object']\n",
    "    # when image has only one bounding box\n",
    "    if not isinstance(items,list):\n",
    "        items = [items]\n",
    "    res = []\n",
    "    for i in items:\n",
    "        n = i['name']\n",
    "        bb = [(int(i['bndbox']['xmin']), int(i['bndbox']['ymin'])), (int(i['bndbox']['xmax']), int(i['bndbox']['ymax']))]\n",
    "        res.append((n,bb))\n",
    "    s = [int(x['annotation']['size']['width']), int(x['annotation']['size']['height'])]\n",
    "    return res, s  # res contains classes & its bounding box's x,y coords; s contains width & height of bounding box\n",
    "def Visualize(imgs, bb=True):\n",
    "    '''\n",
    "    function to visualize the images in the dataset along with appropriate bounding boxes according to their classes\n",
    "    '''\n",
    "    i, l = Path(imgs)\n",
    "    img = cv2.imread(i)  # reading the image using OpenCV's imread function\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) # converting image to RGB format as OpenCV reads image in BGR format\n",
    "    if bb:  # bb (bounding box) is set as true in function parameter\n",
    "        lbls, s = ParseXML(l)\n",
    "        t = int(sum(s)/500)\n",
    "        for lbl in lbls:\n",
    "            n, bb = lbl # lbl has the classes and bound box's x,y coords of result as defined by the above fucntion\n",
    "            if n == 'without_mask' or n == 'bad' : # if mask is not worn, bounding box of red color is drawn\n",
    "                cv2.rectangle(img, bb[0], bb[1], (255, 0, 0), t)\n",
    "            elif n == 'with_mask' or n == 'good':  # if mask is worn correctly, bounding box of green color is drawn\n",
    "                cv2.rectangle(img, bb[0], bb[1], (0, 255, 0), t)\n",
    "            elif n == 'mask_weared_incorrect' or n == 'none': # if mask is worn incorrectly, bounding box of blue color is drawn\n",
    "                cv2.rectangle(img, bb[0], bb[1], (0, 0, 255), t)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = ImageNames() # getting the image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the bounding boxes on a few randomly selected images in the dataaset\n",
    "r = random.sample(range(0,len(imgs)),3)\n",
    "for i in r:\n",
    "    Visualize(imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CropImage(imgs):\n",
    "    '''\n",
    "    function to crop the part of image inside bounding box, and that cropped part will be\n",
    "    fed as input in a pre-trained neural network ResNet-50, to detect the presence of the mask\n",
    "    '''\n",
    "    i, l = Path(imgs)\n",
    "    # image pre-processing\n",
    "    img = cv2.imread(i)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    # getting the labels and bounding boxes' coords and size\n",
    "    lbls, s = ParseXML(l)\n",
    "    cropped_img_lbls = []\n",
    "    for lbl in lbls:\n",
    "        # from labels we extract the label names and annotation parameters\n",
    "        n, bb = lbl\n",
    "        # cropping out the part of image inside bounding box\n",
    "        cropped_img = img[bb[0][1]:bb[1][1], bb[0][0]:bb[1][0]]\n",
    "        lbl_num = 0\n",
    "        # as label contains categorical variables, they are assigned numeric categories\n",
    "        if n == \"with_mask\":\n",
    "            lbl_num = 1\n",
    "        elif n == \"without_mask\":\n",
    "            lbl_num = 2\n",
    "        elif n == \"mask_weared_incorrect\" or n == \"none\":\n",
    "            lbl_num = 3\n",
    "        # getting the cropped image and label\n",
    "        cropped_img_lbl = [cropped_img, lbl_num]\n",
    "        # creating the list of all cropped images exracted from the input images and its label numbers\n",
    "        cropped_img_lbls.append(cropped_img_lbl)\n",
    "    return cropped_img_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train directory with separate folders belonging to separate classes and also the model directory to save the models\n",
    "\n",
    "trn = 'train/'\n",
    "l1 = trn + \"1/\"\n",
    "l2 = trn + \"2/\"\n",
    "l3 = trn + \"3/\"\n",
    "mdl = \"model/\"\n",
    "\n",
    "os.mkdir(trn)\n",
    "os.mkdir(l1)\n",
    "os.mkdir(l2)\n",
    "os.mkdir(l3)\n",
    "os.mkdir(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl1 = 0\n",
    "lbl2 = 0\n",
    "lbl3 = 0\n",
    "for i in imgs:\n",
    "    cropped_img_lbls = CropImage(i)\n",
    "    # iterating through the cropped images and their target labels list\n",
    "    for l in cropped_img_lbls:\n",
    "        # extracting the images and labels from the list\n",
    "        img = l[0]\n",
    "        lbl = l[1]\n",
    "        # renaming the images with their index numbers along with appending their directory names\n",
    "        if lbl == 1:\n",
    "            cropped_img = str(lbl1) + \".png\"\n",
    "            cv2.imwrite(l1 + cropped_img, img)\n",
    "            lbl1 += 1\n",
    "        elif lbl == 2:\n",
    "            cropped_img = str(lbl2) + \".png\"\n",
    "            cv2.imwrite(l2 + cropped_img, img)\n",
    "            lbl2 += 1\n",
    "        elif lbl == 3:\n",
    "            cropped_img = str(lbl3) + \".png\"\n",
    "            cv2.imwrite(l3 + cropped_img, img)\n",
    "            lbl3 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making list of images according to their respective classes\n",
    "lbl_1 = [f for f in os.listdir(l1) if os.path.isfile(os.path.join(l1, f))]  \n",
    "lbl_2 = [f for f in os.listdir(l2) if os.path.isfile(os.path.join(l2, f))]\n",
    "lbl_3 = [f for f in os.listdir(l3) if os.path.isfile(os.path.join(l3, f))]\n",
    "print(\"Total no. of images = \" + str(len(lbl_1)+len(lbl_2)+len(lbl_3)) + \"\\n\")\n",
    "print(\"No. of images labeled 1 = \" + str(len(lbl_1)))\n",
    "print(\"No. of images labeled 2 = \" + str(len(lbl_2)))\n",
    "print(\"No. of images labeled 3 = \" + str(len(lbl_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GPU for training if available else using CPU to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class state(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(state , self).__init__()\n",
    "            \n",
    "        self.cnn1 = nn.Conv2d(in_channels=3 , out_channels=8 , kernel_size = 3 , stride = 1 , padding  = 1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.cnn2 = nn.Conv2d(in_channels=8 , out_channels=16 , kernel_size=3 , stride = 1 , padding = 1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(16)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features=16*56*56 , out_features=4000)\n",
    "        self.dropout = nn.Dropout(0.55)\n",
    "        self.fc2 = nn.Linear(in_features=4000 , out_features=2000)\n",
    "        self.dropout = nn.Dropout(0.55)\n",
    "        self.fc3 = nn.Linear(in_features=2000 , out_features=512)\n",
    "        self.dropout = nn.Dropout(0.45)\n",
    "        self.fc4 = nn.Linear(in_features=512 , out_features=2)\n",
    "        self.final_act = nn.LogSoftmax(dim=1)\n",
    "    def forward(self , x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool2(out)\n",
    "      #  print(out.shape)\n",
    "        out = out.view(-1,16*56*56)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.final_act(out)\n",
    "        return out\n",
    "    \n",
    "model = state()\n",
    "        \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying series of transformation using tranforms.Compose(): it creates a pipeline for series of tranformations needed to apply the the data\n",
    "#resize the image to 224*224 size array\n",
    "#convert into pytorch compatible tensor from nd_array type\n",
    "#normalize and scale the image between [0,1]\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GETTING THE DATA FROM THE FOLDER AND APPLYING THE TRANSFORMATIONS DEFINED ABOVE\n",
    "dataset = datasets.ImageFolder(trn, transform = train_transforms)\n",
    "\n",
    "\n",
    "#dividing into train test and validate data\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.6)\n",
    "val_size = int(dataset_size * 0.2)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "#randomly splitting the data for no class imbalances and model performs robust when trained on random shuffle\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "print('Dataset size: ', len(dataset))\n",
    "print('Train set size: ', len(train_dataset))\n",
    "print('Validation set size: ', len(val_dataset))\n",
    "print('Test set size: ', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "#LOADING THE DATA IN BATCHES FOR COMPUTATION INCLUDING LESS MEMORY CONSUMPTION\n",
    "\n",
    "#loading the train data\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    "\n",
    "#loading the validation data\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    "\n",
    "#loading the test data\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "#using crossentropy loss and adam optimizer(an upgrade to stochastic gradient descent with momentum capabilities)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.cuda()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 50\n",
    "\n",
    "best_epoch = 0\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "\n",
    "#TRAINING PROCESS\n",
    "for epoch in range(total_epoch):\n",
    "    #keep track of training loss\n",
    "    epoch_train_loss = 0\n",
    "    \n",
    "    for X, y in train_loader:\n",
    "        \n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        #setting zero_grad, as in pytorch , optimizer keeps accumulating gradients during each backpropgation\n",
    "        optimizer.zero_grad()\n",
    "        #forward pass\n",
    "        result = model(X)\n",
    "        #calculating the loss\n",
    "        loss = criterion(result, y)\n",
    "        #adding the training loss for better tracking of loss minimization process\n",
    "        epoch_train_loss += loss.item()\n",
    "        #executing backward propagation for calcuation of gradients\n",
    "        loss.backward()\n",
    "        #updating the weights using the calculated gradients \n",
    "        optimizer.step()\n",
    "      \n",
    "    training_losses.append(epoch_train_loss)\n",
    "    \n",
    "    \n",
    "    epoch_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    ##\n",
    "    # A SMALL DESCRIPTION:\n",
    "    # torch.no_grad() :::::>>  it impacts the autograd(backpropagation) engine and deactivates it . It will reduce the memory usage\n",
    "    #                          and speed up the computations and hence we wont be able to perform backprop .\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            \n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            #forward pass\n",
    "            result = model(X)\n",
    "            #calculating the loss\n",
    "            loss = criterion(result, y)\n",
    "            #appending the loss\n",
    "            epoch_val_loss += loss.item()\n",
    "            #extracting the  indeces of maximum values from each row of predictions\n",
    "            #like: if second prediction is hightest, it will return 1 , the index of second prediction\n",
    "            #then we will compare with y which have the correct label for that image with predicted label\n",
    "            _, maximum = torch.max(result.data, 1)\n",
    "            total += y.size(0)\n",
    "            #summing all the correct predictions by matching with the true lables for accuracy calcuation\n",
    "            correct += (maximum == y).sum().item()\n",
    "            \n",
    "    val_losses.append(epoch_val_loss)\n",
    "    #calculating the valildation accuracy\n",
    "    accuracy = correct/total\n",
    "    print(\"EPOCH:\", epoch, \", Training Loss:\", epoch_train_loss, \", Validation Loss:\", epoch_val_loss, \", Accuracy: \", accuracy)\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION OF TRAINING AND VALIDATION LOSS CHANGES THROUGHOUT THE PROCESS\n",
    "plt.plot(range(total_epoch), training_losses, label='Training')\n",
    "plt.plot(range(total_epoch), val_losses, label='Validation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING THE SAVED MODEL\n",
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # AS WE HAVE TO PREFORM TESTING, WE DONT NEED BACKPROPAGATOIN , so setting 'requires_grad' equals FALSE\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "   #RETURNING MODEL IN EVALUATIOIN MODE ::>  .eval() do not change anny behaviour of gradient calculations , but are used to set specific layers like \n",
    "   #                                         dropout and batchnorm to evaluation mode i.e. dropout layer won't drop activations and \n",
    "   #                                         batchnorm will use running estimates instead batch statistics.\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "filepath = models_dir + str(best_epoch) + \".pth\"\n",
    "#loading th model for testing\n",
    "loaded_model = load_checkpoint(filepath)\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                                       transforms.Resize((224,224)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "                                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        \n",
    "        #FORWARD PASS\n",
    "        result = loaded_model(X)\n",
    "        \n",
    "        #ACCURACY CALCULATION\n",
    "        _, maximum = torch.max(result.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (maximum == y).sum().item()\n",
    "\n",
    "accuracy = correct/total\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"---\")\n",
    "print(\"Accuracy: \" + str(accuracy*100))\n",
    "print(\"---\")\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
