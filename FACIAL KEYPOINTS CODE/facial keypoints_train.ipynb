{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import torch\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the annotations file\n",
    "key = pd.read_csv('../input/input-key/training_frames_keypoints.csv')\n",
    "key.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "         \n",
    "        # if image has no grayscale color channel, add one\n",
    "        if(len(image.shape) == 2):\n",
    "            # add that third color dim\n",
    "            image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "            \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        \n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'keypoints': torch.from_numpy(key_pts)}\n",
    "class Normalize(object):      \n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        \n",
    "        image_copy = np.copy(image)\n",
    "        key_pts_copy = np.copy(key_pts)\n",
    "\n",
    "        # convert image to grayscale\n",
    "        image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # scale color range from [0, 255] to [0, 1]\n",
    "        image_copy=  image_copy/255.0\n",
    "        \n",
    "        # scale keypoints to be centered around 0 with a range of [-1, 1]\n",
    "        # mean = 100, sqrt = 50, so, pts should be (pts - 100)/50\n",
    "        key_pts_copy = (key_pts_copy - 100)/50.0\n",
    "\n",
    "\n",
    "        return {'image': image_copy, 'keypoints': key_pts_copy}\n",
    "class Rescale(object):\n",
    "    \n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = cv2.resize(image, (new_w, new_h))\n",
    "        \n",
    "        # scale the pts, too\n",
    "        key_pts = key_pts * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'keypoints': key_pts}\n",
    "class RandomCrop(object):\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        key_pts = key_pts - [left, top]\n",
    "\n",
    "        return {'image': image, 'keypoints': key_pts}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ist column contain name of the file image and all other columns \n",
    "#contain the x and y axis of keypoints\n",
    "#so we will separate them\n",
    "#create a function to make a dataset of form A sample of our dataset will be a dictionary {'image': image, 'keypoints': key_pts}\n",
    "from torch.utils.data import DataLoader , Dataset\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "class facialdataset(Dataset):\n",
    "    def __init__(self,csv_file , root_dir , transform  = None):\n",
    "        self.key_csv = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    #function to return the length of keypoints dictionary\n",
    "    def __len__(self):\n",
    "        return len(self.key_csv)\n",
    "    def __getitem__(self,idx):\n",
    "        #append root dir and image name from csv to fetch image\n",
    "        image_name = os.path.join(self.root_dir , self.key_csv.iloc[idx,0])\n",
    "        image = mpimg.imread(image_name)\n",
    "        #remove last channel if image have 4 channels instead of 3\n",
    "        if image.shape[2] == 4:\n",
    "            image = image[:,:,0:3]\n",
    "        #now convert the cords in matrix and the reshape\n",
    "        key_cords = self.key_csv.iloc[idx,1:].to_numpy()\n",
    "        key_cords = key_cords.astype('float').reshape(-1,2)\n",
    "        dictionary = {'image':image , 'keypoints':key_cords}\n",
    "        \n",
    "        #applying transform is not none:\n",
    "        if self.transform:\n",
    "            dictionary= self.transform(dictionary)\n",
    "        return dictionary\n",
    "# define the data tranform\n",
    "# order matters! i.e. rescaling should come before a smaller crop\n",
    "train_transforms = transforms.Compose([Rescale(250),\n",
    "                                       #ImgAugTransform(),\n",
    "                                        RandomCrop(224),\n",
    "                                         Normalize(),\n",
    "                                       #lambda x: PIL.Image.fromarray(x),\n",
    "                                        #transforms.ToPILImage(),\n",
    "                                          #transforms.RandomVerticalFlip(),\n",
    "                                      ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after creating the class, we will pass the images and labels directory\n",
    "#in the class to get the dictionary\n",
    "face_dataset_train = facialdataset(csv_file='/kaggle/input/input-key/training_frames_keypoints.csv',\n",
    "                            root_dir='/kaggle/input/input-key/training/training/',transform=train_transforms)\n",
    "face_dataset_test = facialdataset(csv_file = '../input/input-key/test_frames_keypoints.csv',\n",
    "                                 root_dir = '../input/input-key/test/test/',\n",
    "                                 transform=train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length of train data' , len(face_dataset_train))\n",
    "print('length of test data' , len(face_dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train\" , face_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of images in train' , len(face_dataset_train))\n",
    "print('number of images in test' , len(face_dataset_test))\n",
    "for i in range(1,5):\n",
    "    sample = face_dataset_test[i]\n",
    "    print(i , sample['image'].size() , sample['keypoints'].size() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#TO INITIALIZE WEIGHTS OF NETWORK\n",
    "import torch.nn.init as I\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # This network takes in a square (same width and height), grayscale image as input\n",
    "        # and it ends with a linear layer that represents the keypoints\n",
    "        # 1 input image channel (grayscale), 32 output channels/feature maps, 5x5 square convolution kernel        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5, 5), stride=2, padding=1) \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(4, 4), stride=2, padding=1) \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(3, 3), stride=1, padding=1)  \n",
    "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=(3, 3), stride=1, padding=1)  \n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=512, kernel_size=(3, 3), stride=1, padding=0)  \n",
    "        self.conv6 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=(3, 3), stride=1, padding=0)  \n",
    "        # maxpooling layers, multiple conv layers, fully-connected layers,\n",
    "        # and other layers (such as dropout) to avoid overfitting\n",
    "        # max-pool layer \n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        # linear layers\n",
    "        self.fc1 = nn.Linear(in_features=4096, out_features=2048)\n",
    "        self.fc2 = nn.Linear(in_features=2048, out_features=512)\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=136)\n",
    "        # dropout \n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.dropout4 = nn.Dropout(p=0.4)\n",
    "        self.dropout6 = nn.Dropout(p=0.6)      \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is the input image\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.elu(self.conv5(x))\n",
    "        x = F.elu(self.conv6(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout2(x)\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1) \n",
    "        # fully connected layers\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)        \n",
    "        # a modified x, having gone through all the layers of your model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "face_model = Net()\n",
    "face_model.to(device)\n",
    "#if torch.cuda.is_available():\n",
    " #   face_model.cuda()\n",
    "print(face_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(face_dataset_train, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, \n",
    "                          )\n",
    "val_loader = DataLoader(face_dataset_test, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True, \n",
    "                          )\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#loss = nn.NLLLoss()\n",
    "loss = nn.SmoothL1Loss()\n",
    "#loss = nn.MSELoss()\n",
    "opt = optim.Adam(params = face_model.parameters() , lr = 0.001)\n",
    "#opt = optim.Adam(params = face_model.parameters() , lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def train_mod(epochs):\n",
    "    \n",
    "    #epochs = 5\n",
    "    correct = 0\n",
    "    face_model.train()\n",
    "    training_loss=[]\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i , data in enumerate(train_loader):\n",
    "            images = data['image'].cuda()\n",
    "           # images = images.cuda()\n",
    "           # print(images.shape)\n",
    "            cords = data['keypoints'].cuda()\n",
    "           # cords = cords.cuda()\n",
    "            \n",
    "            #flattening the points\n",
    "            cords_final = cords.view(cords.size(0),-1)#.cuda()\n",
    "            #as we have to calculate regression loss , so we'll convert into float values\n",
    "            #cords_final = cords_final.type(torch.FloatTensor)\n",
    "            cords_final = cords_final.type(torch.cuda.FloatTensor)\n",
    "            #images = images.type(torch.FloatTensor)\n",
    "            images = images.type(torch.cuda.FloatTensor)\n",
    "            # images = images.view(batch_size,images.shape[0],images.shape[1],images.shape[2])\n",
    "            #print(images.shape)\n",
    "            #forward pass\n",
    "            out = face_model(images)\n",
    "            #loss calculation\n",
    "            loss_ = loss(out,cords_final)\n",
    "            #initializing optimizer\n",
    "            opt.zero_grad()\n",
    "            #backward pass\n",
    "            loss_.backward()\n",
    "            #update weights\n",
    "            opt.step()\n",
    "            #adding loss\n",
    "            running_loss+=loss_.item()\n",
    "           # correct += (out == cords_final).float().sum()\n",
    "        \n",
    "            if i % 10 == 9:    # print every 10 batches\n",
    "                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, i+1, running_loss/10))\n",
    "                accuracy = 100 * correct \n",
    "               # print(\" train Accuracy = {}\".format(accuracy/batch_size))\n",
    "                running_loss = 0\n",
    "               # accuracy=0\n",
    "            training_loss.append(running_loss)\n",
    "            \n",
    "    print('Finished Training')\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = train_mod(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.mkdir(\"/kaggle/working/saved_models\")\n",
    "\n",
    "#creating a dictionary with model parameters for saving\n",
    "checkpoint = {'model': face_model,\n",
    "                            'state_dict': face_model.state_dict(),\n",
    "                        'optimizer' : opt.state_dict()}\n",
    "        \n",
    "        #saving the model\n",
    "torch.save(checkpoint, '/kaggle/working/saved_models/' + 'model0.pth')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the model\n",
    "def test_model():\n",
    "    face_model.eval()\n",
    "    # iterate through the test dataset\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, sample in enumerate(val_loader):\n",
    "        \n",
    "        # get sample data: images and ground truth keypoints\n",
    "            images = sample['image'].cuda()\n",
    "            key_ = sample['keypoints'].cuda()\n",
    "\n",
    "        # convert images to FloatTensors\n",
    "            images = images.type(torch.cuda.FloatTensor)\n",
    "\n",
    "        # forward pass to get net output\n",
    "            pred_key = face_model(images)\n",
    "        \n",
    "        # reshape to batch_size x 68 x 2 pts\n",
    "            pred_key = pred_key.view(pred_key.size()[0], 68, -1)\n",
    "        \n",
    "        # break after first image is tested\n",
    "            if i <20:\n",
    "                return images, pred_key, key_\n",
    "\n",
    "val_img , pred_key , true_key = test_model()\n",
    "\n",
    "print(val_img.data.size())\n",
    "print(pred_key.data.size())\n",
    "print(true_key.size())\n",
    "#print(val_img , pred_key , true_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_keypoints(image, predicted_key_pts, gt_pts=None):\n",
    "    # image is grayscale\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\n",
    "    # plot ground truth points as green pts\n",
    "    if gt_pts is not None:\n",
    "        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=15, marker='.', c='g')\n",
    "\n",
    "\n",
    "\n",
    "def visualize_output(test_images, test_outputs, gt_pts=None, batch_size=5):\n",
    "\n",
    "    for i in range(30):\n",
    "        plt.figure(figsize=(100,100))\n",
    "        ax = plt.subplot(3,25, i+1)\n",
    "\n",
    "        # un-transform the image data\n",
    "        image = test_images[i].cpu().data   # get the image from it's list of images\n",
    "        image = image.numpy()   # convert to numpy array from a Tensor\n",
    "        image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n",
    "\n",
    "        # un-transform the predicted key_pts data\n",
    "        predicted_key_pts = test_outputs[i].cpu().data\n",
    "        predicted_key_pts = predicted_key_pts.numpy()\n",
    "        # undo normalization of keypoints  \n",
    "        predicted_key_pts = predicted_key_pts*50.0+100\n",
    "        \n",
    "        # plot ground truth points for comparison, if they exist\n",
    "        ground_truth_pts = None\n",
    "        if gt_pts is not None:\n",
    "            ground_truth_pts = gt_pts[i]         \n",
    "            ground_truth_pts = ground_truth_pts*50.0+100\n",
    "        \n",
    "        # call show_all_keypoints\n",
    "        show_all_keypoints(np.squeeze(image),predicted_key_pts)\n",
    "            \n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "visualize_output(val_img, pred_key)#, true_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mod(epochs):\n",
    "    \n",
    "    epochs = 2\n",
    "    face_model.eval()\n",
    "    val_loss=0.0\n",
    "    all_val_loss=[]\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i , data in enumerate(val_loader):\n",
    "                images = data['image']\n",
    "               # print(images.shape)\n",
    "                cords = data['keypoints']\n",
    "                #flattening the points\n",
    "                cords_final = cords.view(cords.size(0),-1)\n",
    "                #as we have to calculate regression loss , so we'll convert into float values\n",
    "                cords_final = cords_final.type(torch.cuda.FloatTensor)\n",
    "                images = images.type(torch.cuda.FloatTensor)\n",
    "                # images = images.view(batch_size,images.shape[0],images.shape[1],images.shape[2])\n",
    "                #print(images.shape)\n",
    "                #forward pass\n",
    "                out = face_model(images)\n",
    "                #loss calculation\n",
    "                loss_ = loss(out,cords_final)\n",
    "                val_loss+=loss_.item()\n",
    "                all_val_loss.append(val_loss)\n",
    "                if i % 10 == 9:    # print every 10 batches\n",
    "                    print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, i+1, val_loss/10))\n",
    "                    val_loss=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loss = test_mod(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.mkdir(\"/kaggle/working/models\")\n",
    "\n",
    "#creating a dictionary with model parameters for saving\n",
    "checkpoint = {'model': face_model,\n",
    "                            'state_dict': face_model.state_dict(),\n",
    "                        'optimizer' : opt.state_dict()}\n",
    "        \n",
    "        #saving the model\n",
    "torch.save(checkpoint, '/kaggle/working/models/' + 'model1.pth')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # AS WE HAVE TO PREFORM TESTING, WE DONT NEED BACKPROPAGATOIN , so setting 'requires_grad' equals FALSE\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "   #RETURNING MODEL IN EVALUATIOIN MODE ::>  .eval() do not change anny behaviour of gradient calculations , but are used to set specific layers like \n",
    "   #                                         dropout and batchnorm to evaluation mode i.e. dropout layer won't drop activations and \n",
    "   #                                         batchnorm will use running estimates instead batch statistics.\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "#filepath =  + str(best_epoch) + \".pth\"\n",
    "#loading th model for testing\n",
    "#loaded_model = load_checkpoint(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
